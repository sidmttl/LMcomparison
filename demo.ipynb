{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Generation Demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from attention import *\n",
    "from mamba import *\n",
    "from xlstm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Encoder and Decoder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each model using torch.load\n",
    "gpt_model = torch.load('./models/attention_best_model.pt')\n",
    "mamba_model = torch.load('./models/mamba_best_model.pt')\n",
    "xlstm_model = torch.load('./models/xlstm_best_model.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Based Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(decode(\u001b[43mgpt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "File \u001b[0;32m~/Downloads/nlpp/GPT.py:168\u001b[0m, in \u001b[0;36mGPTLanguageModel.generate\u001b[0;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[1;32m    166\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, C)\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# sample from the distribution\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m idx_next \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, 1)\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# append sampled index to the running sequence\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mprint\u001b[39m(idx_next, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midx_next\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(gpt_model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the night class is their son;\n",
      "Or diege'r sick'd is worp the heart.\n",
      "Fon, you at you thank from justome in,\n",
      "My my chood seem all upon sold talk:\n",
      "I'll use your strangerough own makes to me.\n",
      "\n",
      "COMINIUS:\n",
      "I this is it parith a mayer;\n",
      "As let my lates, what this and come trak he ar\n",
      "To best we have the of butchled childeling?\n",
      "\n",
      "ROMEO:\n",
      "Give at this, not my sight.\n",
      "\n",
      "First Lord Hum! Sir Tobbroth up of arow;\n",
      "And ay, sisure down por 'nwatch Duke,\n",
      "Methan there to empe to Edwards our ilovin;\n",
      "Which I doth generought is thee hoot?\n",
      "To\n"
     ]
    }
   ],
   "source": [
    "# context = torch.zeros(, dtype=torch.long, device=device)\n",
    "print(\"####################### Attention Based Transformer ############################\")\n",
    "context = torch.tensor(encode(\"the night class is\"), dtype=torch.long)\n",
    "context = torch.unsqueeze(context, 0)\n",
    "print(decode(gpt_model.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "\n",
    "print(\"####################### Mamba Based Transformer ############################\")\n",
    "print(decode(mamba_model.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "\n",
    "print(\"################################ xLSTM #####################################\")\n",
    "for _ in range(500):\n",
    "    context, out = xlstm_model.generate(context, max_new_tokens=1)\n",
    "    print(decode(out[0].tolist()), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"####################### Attention Based Transformer ############################\")\n",
    "context = torch.tensor(encode(\"Hello friend\"), dtype=torch.long)\n",
    "context = torch.unsqueeze(context, 0)\n",
    "print(decode(gpt_model.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "\n",
    "print(\"####################### Mamba Based Transformer ############################\")\n",
    "print(decode(mamba_model.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "\n",
    "print(\"####################### xLSTM ############################\")\n",
    "for _ in range(500):\n",
    "    context, out = xlstm_model.generate(context, max_new_tokens=1)\n",
    "    print(decode(out[0].tolist()), end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
