{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-05T18:04:20.997840Z",
     "iopub.status.busy": "2024-08-05T18:04:20.997413Z",
     "iopub.status.idle": "2024-08-05T18:04:21.001368Z",
     "shell.execute_reply": "2024-08-05T18:04:21.000590Z",
     "shell.execute_reply.started": "2024-08-05T18:04:20.997809Z"
    },
    "id": "Lwe-WINzgxo0",
    "outputId": "d0f06df6-868d-44c3-f798-189e7d6cf79e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-05T19:32:37.443583Z",
     "iopub.status.busy": "2024-08-05T19:32:37.443199Z",
     "iopub.status.idle": "2024-08-05T19:32:39.428073Z",
     "shell.execute_reply": "2024-08-05T19:32:39.427401Z",
     "shell.execute_reply.started": "2024-08-05T19:32:37.443556Z"
    },
    "id": "JuJyDEyGH2bG",
    "outputId": "a24bf000-dbf5-4973-f947-115adf62e65e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.cache/pypoetry/virtualenvs/lmcomparison-d-SXeCBZ-py3.10/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:258: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torcheval.metrics.text import Perplexity\n",
    "from tqdm import tqdm\n",
    "from attention import GPTLanguageModel\n",
    "from mamba import MambaLanguageModel\n",
    "from xlstm import XLSTMLanguageModel\n",
    "torch.manual_seed(1337)\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-05T04:10:09.766297Z",
     "iopub.status.busy": "2024-08-05T04:10:09.765898Z",
     "iopub.status.idle": "2024-08-05T04:10:10.433731Z",
     "shell.execute_reply": "2024-08-05T04:10:10.432872Z",
     "shell.execute_reply.started": "2024-08-05T04:10:09.766270Z"
    },
    "id": "tILPyiHesg8s",
    "outputId": "198c0c80-ca8e-4b57-c061-60074f38616d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-05 04:10:10--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.3’\n",
      "\n",
      "input.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
      "\n",
      "2024-08-05 04:10:10 (76.2 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-05T19:32:44.212593Z",
     "iopub.status.busy": "2024-08-05T19:32:44.212110Z",
     "iopub.status.idle": "2024-08-05T19:32:45.185996Z",
     "shell.execute_reply": "2024-08-05T19:32:45.185127Z",
     "shell.execute_reply.started": "2024-08-05T19:32:44.212563Z"
    },
    "id": "f2jFHDf7mNsB",
    "outputId": "37bd1181-3b4b-46a9-b7e4-d43329c41bd1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshetty-sau\u001b[0m (\u001b[33mshetty-sau-northeastern-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T19:32:45.435323Z",
     "iopub.status.busy": "2024-08-05T19:32:45.434860Z",
     "iopub.status.idle": "2024-08-05T19:32:45.441600Z",
     "shell.execute_reply": "2024-08-05T19:32:45.440950Z",
     "shell.execute_reply.started": "2024-08-05T19:32:45.435294Z"
    },
    "id": "Xb-174qiIAwk",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 125 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 500000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "torch.set_default_device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T19:32:47.594092Z",
     "iopub.status.busy": "2024-08-05T19:32:47.593696Z",
     "iopub.status.idle": "2024-08-05T19:32:47.885232Z",
     "shell.execute_reply": "2024-08-05T19:32:47.884491Z",
     "shell.execute_reply.started": "2024-08-05T19:32:47.594063Z"
    },
    "id": "aJfsm47kIUYa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T19:32:50.055355Z",
     "iopub.status.busy": "2024-08-05T19:32:50.054971Z",
     "iopub.status.idle": "2024-08-05T19:32:50.060174Z",
     "shell.execute_reply": "2024-08-05T19:32:50.059347Z",
     "shell.execute_reply.started": "2024-08-05T19:32:50.055327Z"
    },
    "id": "AMZtX5y5IVBN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T19:32:50.404461Z",
     "iopub.status.busy": "2024-08-05T19:32:50.404152Z",
     "iopub.status.idle": "2024-08-05T19:32:50.410122Z",
     "shell.execute_reply": "2024-08-05T19:32:50.409441Z",
     "shell.execute_reply.started": "2024-08-05T19:32:50.404436Z"
    },
    "id": "_Z64w-lUIYE7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        metric = Perplexity()\n",
    "        metric.to(device)\n",
    "        metric.reset()\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            BT, C = logits.shape\n",
    "            logits = logits.view(batch_size, BT//batch_size, C)\n",
    "            Y = Y.view(batch_size, BT//batch_size)\n",
    "            metric.update(logits, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "        out[str(split)+\"_perplexity\"] = metric.compute()\n",
    "        del metric\n",
    "    model.train()\n",
    "    return out\n",
    "    \n",
    "# @torch.no_grad()\n",
    "# def estimate_loss():\n",
    "#     out = {}\n",
    "#     model.eval()\n",
    "#     for split in ['train', 'val']:\n",
    "#         losses = torch.zeros(eval_iters)\n",
    "#         for k in range(eval_iters):\n",
    "#             X, Y = get_batch(split)\n",
    "#             logits, loss = model(X, Y)\n",
    "#             losses[k] = loss.item()\n",
    "#         out[split] = losses.mean()\n",
    "\n",
    "#     for split in ['perplexity_train', 'perplexity_val']:\n",
    "#         metric = Perplexity()\n",
    "#         metric.to(device)\n",
    "#         metric.reset()\n",
    "#         for k in range(eval_iters):\n",
    "#             X, Y = get_batch(split)\n",
    "#             logits, loss = model(X, Y)\n",
    "#             BT, C = logits.shape\n",
    "#             logits = logits.view(batch_size, BT//batch_size, C)\n",
    "#             Y = Y.view(batch_size, BT//batch_size)\n",
    "#             metric.update(logits, Y)\n",
    "#         out[split] = metric.compute()\n",
    "#         del metric\n",
    "#     model.train()\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "execution": {
     "iopub.execute_input": "2024-08-05T18:04:41.872067Z",
     "iopub.status.busy": "2024-08-05T18:04:41.871699Z",
     "iopub.status.idle": "2024-08-05T18:04:43.883194Z",
     "shell.execute_reply": "2024-08-05T18:04:43.880504Z",
     "shell.execute_reply.started": "2024-08-05T18:04:41.872038Z"
    },
    "id": "nF_wlf4gIkZL",
    "outputId": "5633c5ad-1bdd-4eed-f386-912c73c8dede",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/workspace/LMcomparison/wandb/run-20240805_180442-ef5qsuts</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shetty-sau-northeastern-university/language-model/runs/ef5qsuts' target=\"_blank\">attention-based-transformers</a></strong> to <a href='https://wandb.ai/shetty-sau-northeastern-university/language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shetty-sau-northeastern-university/language-model' target=\"_blank\">https://wandb.ai/shetty-sau-northeastern-university/language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shetty-sau-northeastern-university/language-model/runs/ef5qsuts' target=\"_blank\">https://wandb.ai/shetty-sau-northeastern-university/language-model/runs/ef5qsuts</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-05T18:04:48.405613Z",
     "iopub.status.busy": "2024-08-05T18:04:48.405092Z",
     "iopub.status.idle": "2024-08-05T19:20:01.259918Z",
     "shell.execute_reply": "2024-08-05T19:20:01.258885Z",
     "shell.execute_reply.started": "2024-08-05T18:04:48.405582Z"
    },
    "id": "vzvDRsHhwWNY",
    "outputId": "c3b8f0af-6856-4b58-bc41-76e7ecead0f0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3219, val loss 4.3331 train_perplexity 75.3341, val_perplexity  76.1789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 500/500000 [09:07<127:21:10,  1.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 1.5949, val loss 1.7762 train_perplexity 4.9279, val_perplexity  5.9072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1000/500000 [18:17<126:32:39,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 1.3015, val loss 1.5502 train_perplexity 3.6746, val_perplexity  4.7123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1500/500000 [27:27<126:05:20,  1.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1500: train loss 1.1783, val loss 1.4867 train_perplexity 3.2488, val_perplexity  4.4225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2000/500000 [36:36<126:20:17,  1.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000: train loss 1.0862, val loss 1.4884 train_perplexity 2.9629, val_perplexity  4.4302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2500/500000 [45:43<125:36:55,  1.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2500: train loss 1.0016, val loss 1.5003 train_perplexity 2.7227, val_perplexity  4.4831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3000/500000 [54:50<125:34:17,  1.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000: train loss 0.9117, val loss 1.5317 train_perplexity 2.4887, val_perplexity  4.6260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3500/500000 [1:03:56<124:56:34,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3500: train loss 0.8318, val loss 1.5840 train_perplexity 2.2974, val_perplexity  4.8742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4000/500000 [1:15:12<155:26:31,  1.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000: train loss 0.7435, val loss 1.6289 train_perplexity 2.1032, val_perplexity  5.0982\n",
      "Early stopping at iteration 4000 with best val loss 1.4867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters for early stopping\n",
    "wandb.watch(model, optimizer, log=\"all\", log_freq=100)\n",
    "\n",
    "patience = 5\n",
    "min_delta = 0.001\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for iter in tqdm(range(max_iters)):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f} train_perplexity {losses['train_perplexity']:.4f}, val_perplexity  {losses['val_perplexity']:.4f}\")\n",
    "        wandb.log({\"train_loss\": losses['train'], \"val_loss\": losses['val'], \"train_perplexity\":losses['train_perplexity'], \"val_perplexity\":losses['val_perplexity']})\n",
    "\n",
    "        # Early stopping check\n",
    "        if losses['val'] < best_val_loss - min_delta:\n",
    "            best_val_loss = losses['val']\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at iteration {iter} with best val loss {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-05T19:20:01.262006Z",
     "iopub.status.busy": "2024-08-05T19:20:01.261572Z",
     "iopub.status.idle": "2024-08-05T19:20:12.974155Z",
     "shell.execute_reply": "2024-08-05T19:20:12.973296Z",
     "shell.execute_reply.started": "2024-08-05T19:20:01.261970Z"
    },
    "id": "3uqmC_0oIzOM",
    "outputId": "7634dd0d-6066-4379-8fe2-eb656f0e5f5e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Black, pioous, use! my gracious right!\n",
      "That fearful worms' tears will under dryck.\n",
      "\n",
      "ApoLIXENES:\n",
      "Pray you, then, let's to see.\n",
      "\n",
      "ESCALUS:\n",
      "Where is threat joy? why, sir?\n",
      "\n",
      "Clown:\n",
      "Army is't like a sect father's enemy.\n",
      "\n",
      "ANGELO:\n",
      "You smell of spoken?\n",
      "\n",
      "ESCALUS:\n",
      "No. Well, sir, I like it.\n",
      "\n",
      "LUCIO:\n",
      "Good villana.\n",
      "\n",
      "POMPEY:\n",
      "Pray, daught you think; and, I can, if you, pray \n",
      "by unrespect, or any of the packer is life, I thou shall\n",
      "goe of your corner, you'll go jest me to unto the noble\n",
      "of my comfort, and lear me \n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8Wc19qngqBs"
   },
   "source": [
    "# MAMBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T19:32:09.524555Z",
     "iopub.status.busy": "2024-08-05T19:32:09.524169Z",
     "iopub.status.idle": "2024-08-05T19:32:09.547755Z",
     "shell.execute_reply": "2024-08-05T19:32:09.546616Z",
     "shell.execute_reply.started": "2024-08-05T19:32:09.524527Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# clear GPU memory\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m m\n\u001b[1;32m      4\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# clear GPU memory\n",
    "del model\n",
    "del m\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T19:33:00.909400Z",
     "iopub.status.busy": "2024-08-05T19:33:00.908998Z",
     "iopub.status.idle": "2024-08-05T19:33:01.219044Z",
     "shell.execute_reply": "2024-08-05T19:33:01.217723Z",
     "shell.execute_reply.started": "2024-08-05T19:33:00.909372Z"
    },
    "id": "6fRco6fHh0OG",
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m n_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m     11\u001b[0m dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMambaLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meval_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mn_embd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_embd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_head\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43mn_layer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m m \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print the number of parameters in the model\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/LMcomparison/mamba/mamba_model.py:23\u001b[0m, in \u001b[0;36mMambaLanguageModel.__init__\u001b[0;34m(self, vocab_size, batch_size, block_size, max_iters, eval_interval, learning_rate, n_embd, n_head, n_layer, dropout)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding_table \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_embd)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_embd)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential([MambaBlock(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_embd, d_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, d_conv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layer)])\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f \u001b[38;5;241m=\u001b[39m RMSNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_embd) \u001b[38;5;66;03m# final layer norm\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_embd, vocab_size)\n",
      "File \u001b[0;32m~/workspace/LMcomparison/mamba/mamba_model.py:23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding_table \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_embd)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_embd)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential([\u001b[43mMambaBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_embd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_conv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layer)])\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f \u001b[38;5;241m=\u001b[39m RMSNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_embd) \u001b[38;5;66;03m# final layer norm\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_embd, vocab_size)\n",
      "File \u001b[0;32m~/workspace/LMcomparison/mamba/mamba_block.py:61\u001b[0m, in \u001b[0;36mMambaBlock.__init__\u001b[0;34m(self, d_model, d_state, d_conv, expand, dt_rank, bias, conv_bias)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv1d(\n\u001b[1;32m     52\u001b[0m     in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_inner,\n\u001b[1;32m     53\u001b[0m     out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_inner,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     padding\u001b[38;5;241m=\u001b[39md_conv \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     58\u001b[0m )\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# x_proj takes in `x` and outputs the input-specific Δ, B, C\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_inner, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt_rank\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43md_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# dt_proj projects Δ from dt_rank to d_in\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt_rank, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_inner, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "batch_size = 125 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 500000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "model = MambaLanguageModel(vocab_size, batch_size = batch_size, block_size = block_size,\n",
    "                max_iters = max_iters, eval_interval = eval_interval, learning_rate = learning_rate,\n",
    "                n_embd = n_embd, n_head = n_head,\n",
    "                n_layer = n_layer, dropout = dropout)\n",
    "\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-05T04:16:53.389778Z",
     "iopub.status.idle": "2024-08-05T04:16:53.390179Z",
     "shell.execute_reply": "2024-08-05T04:16:53.390032Z",
     "shell.execute_reply.started": "2024-08-05T04:16:53.390019Z"
    },
    "id": "EVJdBTTgzDGD"
   },
   "outputs": [],
   "source": [
    "wandb.watch(model, optimizer, log=\"all\", log_freq=100)\n",
    "\n",
    "# Parameters for early stopping\n",
    "patience = 5\n",
    "min_delta = 0.001\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for iter in tqdm(range(max_iters)):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f} train_perplexity {losses['train_perplexity']:.4f}, val_perplexity  {losses['val_perplexity']:.4f}\")\n",
    "        wandb.log({\"train_loss\": losses['train'], \"val_loss\": losses['val'], \"train_perplexity\":losses['train_perplexity'], \"val_perplexity\":losses['val_perplexity']})\n",
    "\n",
    "        # Early stopping check\n",
    "        if losses['val'] < best_val_loss - min_delta:\n",
    "            best_val_loss = losses['val']\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at iteration {iter} with best val loss {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2024-08-05T04:16:53.392098Z",
     "iopub.status.idle": "2024-08-05T04:16:53.392420Z",
     "shell.execute_reply": "2024-08-05T04:16:53.392270Z",
     "shell.execute_reply.started": "2024-08-05T04:16:53.392253Z"
    },
    "id": "Cq713wR6iwL_",
    "outputId": "701c17dc-8433-42a0-a38b-b4720770305e"
   },
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQozQLnZNp7e"
   },
   "source": [
    "# xLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear GPU memory\n",
    "del model\n",
    "delet m\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243,
     "referenced_widgets": [
      "7fdd49eea2c2476c8bd8214bfc20e126",
      "d13fc0e4dee74587aafebc49bc194ad9",
      "2c58e61164964dfe87fac00f42a451c8",
      "3a2c51080f374fcc8eeac36092e79278",
      "a70b3e1e1a2c4d00a72ac242af8600af",
      "b4da1881f2a74b05ac8ec88563ca14dc",
      "5cee93af2987419ab8e0100f68db5400",
      "e1a342ab401a4616a02b0ebbdd5d0534"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-08-05T19:35:58.864425Z",
     "iopub.status.busy": "2024-08-05T19:35:58.864042Z",
     "iopub.status.idle": "2024-08-05T19:35:59.986615Z",
     "shell.execute_reply": "2024-08-05T19:35:59.985622Z",
     "shell.execute_reply.started": "2024-08-05T19:35:58.864399Z"
    },
    "id": "jok3iZlAi0Gl",
    "outputId": "7b4791d8-3dfd-445d-b5c2-19ba9bc8fe39",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/workspace/LMcomparison/wandb/run-20240805_193558-kazd0ohn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shetty-sau-northeastern-university/language-model/runs/kazd0ohn' target=\"_blank\">xlstm-based-transformers</a></strong> to <a href='https://wandb.ai/shetty-sau-northeastern-university/language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shetty-sau-northeastern-university/language-model' target=\"_blank\">https://wandb.ai/shetty-sau-northeastern-university/language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shetty-sau-northeastern-university/language-model/runs/kazd0ohn' target=\"_blank\">https://wandb.ai/shetty-sau-northeastern-university/language-model/runs/kazd0ohn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.902017 M parameters\n"
     ]
    }
   ],
   "source": [
    "batch_size = 125 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 500000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "layer = ['m', 'm', 's']\n",
    "dropout = 0.2\n",
    "torch.set_default_device(device)\n",
    "\n",
    "x = torch.zeros(batch_size, block_size, n_embd)\n",
    "\n",
    "model = XLSTMLanguageModel(vocab_size, x, batch_size = batch_size, block_size = block_size,\n",
    "                max_iters = max_iters, eval_interval = eval_interval, learning_rate = learning_rate,\n",
    "                device = device, eval_iters = eval_interval, dropout = dropout, layers=layer)\n",
    "\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-05T19:36:00.980742Z",
     "iopub.status.busy": "2024-08-05T19:36:00.980216Z",
     "iopub.status.idle": "2024-08-06T00:33:10.604294Z",
     "shell.execute_reply": "2024-08-06T00:33:10.603327Z",
     "shell.execute_reply.started": "2024-08-05T19:36:00.980713Z"
    },
    "id": "QA-4q60JNp7f",
    "outputId": "708dda6e-e806-4111-ba78-14a6cb2ea99b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3027, val loss 4.3005 train_perplexity 73.8967, val_perplexity  73.7376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 500/500000 [22:35<368:48:33,  2.66s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 2.5712, val loss 2.6183 train_perplexity 13.0820, val_perplexity  13.7124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1000/500000 [45:12<369:18:06,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 2.5744, val loss 2.6856 train_perplexity 13.1240, val_perplexity  14.6672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1500/500000 [1:07:50<371:11:40,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1500: train loss 2.5180, val loss 2.5409 train_perplexity 12.4032, val_perplexity  12.6912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2000/500000 [1:30:25<368:01:57,  2.66s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000: train loss 2.4826, val loss 2.5061 train_perplexity 11.9718, val_perplexity  12.2566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2500/500000 [1:52:59<366:28:12,  2.65s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2500: train loss 2.4678, val loss 2.4945 train_perplexity 11.7962, val_perplexity  12.1162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3000/500000 [2:15:35<368:01:42,  2.67s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000: train loss 2.4610, val loss 2.4873 train_perplexity 11.7168, val_perplexity  12.0292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3500/500000 [2:38:13<370:54:50,  2.69s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3500: train loss 2.4587, val loss 2.4900 train_perplexity 11.6892, val_perplexity  12.0613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4000/500000 [3:00:53<367:15:13,  2.67s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000: train loss 2.4577, val loss 2.4839 train_perplexity 11.6779, val_perplexity  11.9879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4500/500000 [3:23:32<367:04:14,  2.67s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4500: train loss 2.4563, val loss 2.4838 train_perplexity 11.6615, val_perplexity  11.9863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5000/500000 [3:46:11<369:01:13,  2.68s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5000: train loss 2.4559, val loss 2.4838 train_perplexity 11.6574, val_perplexity  11.9871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5500/500000 [4:08:48<365:52:58,  2.66s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5500: train loss 2.4566, val loss 2.4854 train_perplexity 11.6654, val_perplexity  12.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6000/500000 [4:31:24<365:59:29,  2.67s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6000: train loss 2.4553, val loss 2.4839 train_perplexity 11.6503, val_perplexity  11.9885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 6500/500000 [4:57:09<376:01:18,  2.74s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6500: train loss 2.4554, val loss 2.4849 train_perplexity 11.6506, val_perplexity  11.9999\n",
      "Early stopping at iteration 6500 with best val loss 2.4839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wandb.watch(model, optimizer, log=\"all\", log_freq=100)\n",
    "\n",
    "# Parameters for early stopping\n",
    "patience = 5\n",
    "min_delta = 0.001\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for iter in tqdm(range(max_iters)):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f} train_perplexity {losses['train_perplexity']:.4f}, val_perplexity  {losses['val_perplexity']:.4f}\")\n",
    "        wandb.log({\"train_loss\": losses['train'], \"val_loss\": losses['val'], \"train_perplexity\":losses['train_perplexity'], \"val_perplexity\":losses['val_perplexity']})\n",
    "\n",
    "\n",
    "        # Early stopping check\n",
    "        if losses['val'] < best_val_loss - min_delta:\n",
    "            best_val_loss = losses['val']\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at iteration {iter} with best val loss {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T00:33:52.714312Z",
     "iopub.status.busy": "2024-08-06T00:33:52.713934Z",
     "iopub.status.idle": "2024-08-06T00:34:09.688960Z",
     "shell.execute_reply": "2024-08-06T00:34:09.687597Z",
     "shell.execute_reply.started": "2024-08-06T00:33:52.714283Z"
    },
    "id": "jNG3OROhNp7g",
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# generate from the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m     10\u001b[0m itos \u001b[38;5;241m=\u001b[39m { i:ch \u001b[38;5;28;01mfor\u001b[39;00m i,ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chars) }\n\u001b[1;32m     11\u001b[0m encode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m s: [stoi[c] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m s] \u001b[38;5;66;03m# encoder: take a string, output a list of integers\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m l: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([itos[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m l]) \u001b[38;5;66;03m# decoder: take a list of integers, output a string\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train and test splits\u001b[39;00m\n\u001b[1;32m     15\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(encode(text), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m itos \u001b[38;5;241m=\u001b[39m { i:ch \u001b[38;5;28;01mfor\u001b[39;00m i,ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chars) }\n\u001b[1;32m     11\u001b[0m encode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m s: [stoi[c] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m s] \u001b[38;5;66;03m# encoder: take a string, output a list of integers\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m l: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[43mitos\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m l]) \u001b[38;5;66;03m# decoder: take a list of integers, output a string\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train and test splits\u001b[39;00m\n\u001b[1;32m     15\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(encode(text), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2c58e61164964dfe87fac00f42a451c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5cee93af2987419ab8e0100f68db5400",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e1a342ab401a4616a02b0ebbdd5d0534",
      "value": 1
     }
    },
    "3a2c51080f374fcc8eeac36092e79278": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5cee93af2987419ab8e0100f68db5400": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fdd49eea2c2476c8bd8214bfc20e126": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d13fc0e4dee74587aafebc49bc194ad9",
       "IPY_MODEL_2c58e61164964dfe87fac00f42a451c8"
      ],
      "layout": "IPY_MODEL_3a2c51080f374fcc8eeac36092e79278"
     }
    },
    "a70b3e1e1a2c4d00a72ac242af8600af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4da1881f2a74b05ac8ec88563ca14dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d13fc0e4dee74587aafebc49bc194ad9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a70b3e1e1a2c4d00a72ac242af8600af",
      "placeholder": "​",
      "style": "IPY_MODEL_b4da1881f2a74b05ac8ec88563ca14dc",
      "value": "0.012 MB of 0.012 MB uploaded\r"
     }
    },
    "e1a342ab401a4616a02b0ebbdd5d0534": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
