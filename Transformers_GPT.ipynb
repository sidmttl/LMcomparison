{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-05T04:10:07.596429Z",
     "iopub.status.busy": "2024-08-05T04:10:07.596217Z",
     "iopub.status.idle": "2024-08-05T04:10:07.599873Z",
     "shell.execute_reply": "2024-08-05T04:10:07.599184Z",
     "shell.execute_reply.started": "2024-08-05T04:10:07.596406Z"
    },
    "id": "Lwe-WINzgxo0",
    "outputId": "d0f06df6-868d-44c3-f798-189e7d6cf79e"
   },
   "outputs": [],
   "source": [
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuJyDEyGH2bG",
    "outputId": "a24bf000-dbf5-4973-f947-115adf62e65e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torcheval.metrics.text import Perplexity\n",
    "from tqdm import tqdm\n",
    "from attention import GPTLanguageModel\n",
    "from mamba import MambaLanguageModel\n",
    "from xlstm import XLSTMLanguageModel\n",
    "torch.manual_seed(1337)\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-05T04:10:09.766297Z",
     "iopub.status.busy": "2024-08-05T04:10:09.765898Z",
     "iopub.status.idle": "2024-08-05T04:10:10.433731Z",
     "shell.execute_reply": "2024-08-05T04:10:10.432872Z",
     "shell.execute_reply.started": "2024-08-05T04:10:09.766270Z"
    },
    "id": "tILPyiHesg8s",
    "outputId": "198c0c80-ca8e-4b57-c061-60074f38616d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-05 04:10:10--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.3’\n",
      "\n",
      "input.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
      "\n",
      "2024-08-05 04:10:10 (76.2 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-05T04:10:10.435495Z",
     "iopub.status.busy": "2024-08-05T04:10:10.435064Z",
     "iopub.status.idle": "2024-08-05T04:10:11.423770Z",
     "shell.execute_reply": "2024-08-05T04:10:11.423059Z",
     "shell.execute_reply.started": "2024-08-05T04:10:10.435463Z"
    },
    "id": "f2jFHDf7mNsB",
    "outputId": "37bd1181-3b4b-46a9-b7e4-d43329c41bd1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshetty-sau\u001b[0m (\u001b[33mshetty-sau-northeastern-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T04:10:11.425778Z",
     "iopub.status.busy": "2024-08-05T04:10:11.425345Z",
     "iopub.status.idle": "2024-08-05T04:10:11.432275Z",
     "shell.execute_reply": "2024-08-05T04:10:11.431526Z",
     "shell.execute_reply.started": "2024-08-05T04:10:11.425751Z"
    },
    "id": "Xb-174qiIAwk",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 125 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 500000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "torch.set_default_device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T04:10:11.433575Z",
     "iopub.status.busy": "2024-08-05T04:10:11.433160Z",
     "iopub.status.idle": "2024-08-05T04:10:11.736570Z",
     "shell.execute_reply": "2024-08-05T04:10:11.735867Z",
     "shell.execute_reply.started": "2024-08-05T04:10:11.433548Z"
    },
    "id": "aJfsm47kIUYa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T04:10:11.739202Z",
     "iopub.status.busy": "2024-08-05T04:10:11.738889Z",
     "iopub.status.idle": "2024-08-05T04:10:11.743815Z",
     "shell.execute_reply": "2024-08-05T04:10:11.743176Z",
     "shell.execute_reply.started": "2024-08-05T04:10:11.739175Z"
    },
    "id": "AMZtX5y5IVBN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T04:10:11.745279Z",
     "iopub.status.busy": "2024-08-05T04:10:11.744855Z",
     "iopub.status.idle": "2024-08-05T04:10:11.752752Z",
     "shell.execute_reply": "2024-08-05T04:10:11.751881Z",
     "shell.execute_reply.started": "2024-08-05T04:10:11.745253Z"
    },
    "id": "_Z64w-lUIYE7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        metric = Perplexity()\n",
    "        metric.to(device)\n",
    "        metric.reset()\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            BT, C = logits.shape\n",
    "            logits = logits.view(batch_size, BT//batch_size, C)\n",
    "            Y = Y.view(batch_size, BT//batch_size)\n",
    "            metric.update(logits, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "        out[str(split)+\"_perplexity\"] = metric.compute()\n",
    "        del metric\n",
    "    model.train()\n",
    "    return out\n",
    "    \n",
    "# @torch.no_grad()\n",
    "# def estimate_loss():\n",
    "#     out = {}\n",
    "#     model.eval()\n",
    "#     for split in ['train', 'val']:\n",
    "#         losses = torch.zeros(eval_iters)\n",
    "#         for k in range(eval_iters):\n",
    "#             X, Y = get_batch(split)\n",
    "#             logits, loss = model(X, Y)\n",
    "#             losses[k] = loss.item()\n",
    "#         out[split] = losses.mean()\n",
    "\n",
    "#     for split in ['perplexity_train', 'perplexity_val']:\n",
    "#         metric = Perplexity()\n",
    "#         metric.to(device)\n",
    "#         metric.reset()\n",
    "#         for k in range(eval_iters):\n",
    "#             X, Y = get_batch(split)\n",
    "#             logits, loss = model(X, Y)\n",
    "#             BT, C = logits.shape\n",
    "#             logits = logits.view(batch_size, BT//batch_size, C)\n",
    "#             Y = Y.view(batch_size, BT//batch_size)\n",
    "#             metric.update(logits, Y)\n",
    "#         out[split] = metric.compute()\n",
    "#         del metric\n",
    "#     model.train()\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "execution": {
     "iopub.execute_input": "2024-08-05T04:10:11.754152Z",
     "iopub.status.busy": "2024-08-05T04:10:11.753797Z",
     "iopub.status.idle": "2024-08-05T04:10:12.890952Z",
     "shell.execute_reply": "2024-08-05T04:10:12.890195Z",
     "shell.execute_reply.started": "2024-08-05T04:10:11.754119Z"
    },
    "id": "nF_wlf4gIkZL",
    "outputId": "5633c5ad-1bdd-4eed-f386-912c73c8dede",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/workspace/LMcomparison/wandb/run-20240805_041011-wekos40a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shetty-sau-northeastern-university/language-model/runs/wekos40a' target=\"_blank\">attention-based-transformers</a></strong> to <a href='https://wandb.ai/shetty-sau-northeastern-university/language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shetty-sau-northeastern-university/language-model' target=\"_blank\">https://wandb.ai/shetty-sau-northeastern-university/language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shetty-sau-northeastern-university/language-model/runs/wekos40a' target=\"_blank\">https://wandb.ai/shetty-sau-northeastern-university/language-model/runs/wekos40a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-05T04:16:59.072663Z",
     "iopub.status.busy": "2024-08-05T04:16:59.072277Z"
    },
    "id": "vzvDRsHhwWNY",
    "outputId": "c3b8f0af-6856-4b58-bc41-76e7ecead0f0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.8867, val loss 2.0116 train_perplexity 6.5975, val_perplexity - 7.4750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 500/500000 [08:52<123:31:42,  1.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 1.3665, val loss 1.5937 train_perplexity 3.9216, val_perplexity - 4.9220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1000/500000 [17:45<123:37:08,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 1.2162, val loss 1.5129 train_perplexity 3.3745, val_perplexity - 4.5401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1500/500000 [26:36<122:37:36,  1.13it/s] "
     ]
    }
   ],
   "source": [
    "# Parameters for early stopping\n",
    "wandb.watch(model, optimizer, log=\"all\", log_freq=100)\n",
    "\n",
    "patience = 5\n",
    "min_delta = 0.001\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for iter in tqdm(range(max_iters)):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f} train_perplexity {losses['train_perplexity']:.4f}, val_perplexity  {losses['val_perplexity']:.4f}\")\n",
    "        wandb.log({\"train_loss\": losses['train'], \"val_loss\": losses['val'], \"train_perplexity\":losses['train_perplexity'], \"val_perplexity\":losses['val_perplexity']})\n",
    "\n",
    "        # Early stopping check\n",
    "        if losses['val'] < best_val_loss - min_delta:\n",
    "            best_val_loss = losses['val']\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at iteration {iter} with best val loss {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uqmC_0oIzOM",
    "outputId": "7634dd0d-6066-4379-8fe2-eb656f0e5f5e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8Wc19qngqBs"
   },
   "source": [
    "# MAMBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear GPU memory\n",
    "del model\n",
    "delet m\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-05T04:16:53.388387Z",
     "iopub.status.idle": "2024-08-05T04:16:53.388901Z",
     "shell.execute_reply": "2024-08-05T04:16:53.388768Z",
     "shell.execute_reply.started": "2024-08-05T04:16:53.388754Z"
    },
    "id": "6fRco6fHh0OG"
   },
   "outputs": [],
   "source": [
    "batch_size = 125 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 500000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "model = MambaLanguageModel(vocab_size, batch_size = batch_size, block_size = block_size,\n",
    "                max_iters = max_iters, eval_interval = eval_interval, learning_rate = learning_rate,\n",
    "                n_embd = n_embd, n_head = n_head,\n",
    "                n_layer = n_layer, dropout = dropout)\n",
    "\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-05T04:16:53.389778Z",
     "iopub.status.idle": "2024-08-05T04:16:53.390179Z",
     "shell.execute_reply": "2024-08-05T04:16:53.390032Z",
     "shell.execute_reply.started": "2024-08-05T04:16:53.390019Z"
    },
    "id": "EVJdBTTgzDGD"
   },
   "outputs": [],
   "source": [
    "wandb.watch(model, optimizer, log=\"all\", log_freq=100)\n",
    "\n",
    "# Parameters for early stopping\n",
    "patience = 5\n",
    "min_delta = 0.001\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for iter in tqdm(range(max_iters)):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f} train_perplexity {losses['train_perplexity']:.4f}, val_perplexity  {losses['val_perplexity']:.4f}\")\n",
    "        wandb.log({\"train_loss\": losses['train'], \"val_loss\": losses['val'], \"train_perplexity\":losses['train_perplexity'], \"val_perplexity\":losses['val_perplexity']})\n",
    "\n",
    "        # Early stopping check\n",
    "        if losses['val'] < best_val_loss - min_delta:\n",
    "            best_val_loss = losses['val']\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at iteration {iter} with best val loss {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2024-08-05T04:16:53.392098Z",
     "iopub.status.idle": "2024-08-05T04:16:53.392420Z",
     "shell.execute_reply": "2024-08-05T04:16:53.392270Z",
     "shell.execute_reply.started": "2024-08-05T04:16:53.392253Z"
    },
    "id": "Cq713wR6iwL_",
    "outputId": "701c17dc-8433-42a0-a38b-b4720770305e"
   },
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQozQLnZNp7e"
   },
   "source": [
    "# xLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear GPU memory\n",
    "del model\n",
    "delet m\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243,
     "referenced_widgets": [
      "7fdd49eea2c2476c8bd8214bfc20e126",
      "d13fc0e4dee74587aafebc49bc194ad9",
      "2c58e61164964dfe87fac00f42a451c8",
      "3a2c51080f374fcc8eeac36092e79278",
      "a70b3e1e1a2c4d00a72ac242af8600af",
      "b4da1881f2a74b05ac8ec88563ca14dc",
      "5cee93af2987419ab8e0100f68db5400",
      "e1a342ab401a4616a02b0ebbdd5d0534"
     ]
    },
    "execution": {
     "iopub.status.busy": "2024-08-05T04:16:53.394604Z",
     "iopub.status.idle": "2024-08-05T04:16:53.394933Z",
     "shell.execute_reply": "2024-08-05T04:16:53.394810Z",
     "shell.execute_reply.started": "2024-08-05T04:16:53.394797Z"
    },
    "id": "jok3iZlAi0Gl",
    "outputId": "7b4791d8-3dfd-445d-b5c2-19ba9bc8fe39"
   },
   "outputs": [],
   "source": [
    "batch_size = 125 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 500000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "layer = ['m', 'm', 's']\n",
    "dropout = 0.2\n",
    "torch.set_default_device(device)\n",
    "\n",
    "x = torch.zeros(batch_size, block_size, n_embd)\n",
    "\n",
    "model = XLSTMLanguageModel(vocab_size, x, batch_size = batch_size, block_size = block_size,\n",
    "                max_iters = max_iters, eval_interval = eval_interval, learning_rate = learning_rate,\n",
    "                device = device, eval_iters = eval_interval, dropout = dropout, layers=layer)\n",
    "\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2024-08-05T04:16:53.396082Z",
     "iopub.status.idle": "2024-08-05T04:16:53.396348Z",
     "shell.execute_reply": "2024-08-05T04:16:53.396230Z",
     "shell.execute_reply.started": "2024-08-05T04:16:53.396219Z"
    },
    "id": "QA-4q60JNp7f",
    "outputId": "708dda6e-e806-4111-ba78-14a6cb2ea99b"
   },
   "outputs": [],
   "source": [
    "wandb.watch(model, optimizer, log=\"all\", log_freq=100)\n",
    "\n",
    "# Parameters for early stopping\n",
    "patience = 5\n",
    "min_delta = 0.001\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for iter in tqdm(range(max_iters)):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f} train_perplexity {losses['train_perplexity']:.4f}, val_perplexity  {losses['val_perplexity']:.4f}\")\n",
    "        wandb.log({\"train_loss\": losses['train'], \"val_loss\": losses['val'], \"train_perplexity\":losses['train_perplexity'], \"val_perplexity\":losses['val_perplexity']})\n",
    "\n",
    "\n",
    "        # Early stopping check\n",
    "        if losses['val'] < best_val_loss - min_delta:\n",
    "            best_val_loss = losses['val']\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at iteration {iter} with best val loss {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-05T04:16:53.397345Z",
     "iopub.status.idle": "2024-08-05T04:16:53.397962Z",
     "shell.execute_reply": "2024-08-05T04:16:53.397742Z",
     "shell.execute_reply.started": "2024-08-05T04:16:53.397719Z"
    },
    "id": "jNG3OROhNp7g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _ in range(500):\n",
    "    context, out = model.generate(context, max_new_tokens=1)\n",
    "    print(decode(out[0].tolist()), end=\"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2c58e61164964dfe87fac00f42a451c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5cee93af2987419ab8e0100f68db5400",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e1a342ab401a4616a02b0ebbdd5d0534",
      "value": 1
     }
    },
    "3a2c51080f374fcc8eeac36092e79278": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5cee93af2987419ab8e0100f68db5400": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fdd49eea2c2476c8bd8214bfc20e126": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d13fc0e4dee74587aafebc49bc194ad9",
       "IPY_MODEL_2c58e61164964dfe87fac00f42a451c8"
      ],
      "layout": "IPY_MODEL_3a2c51080f374fcc8eeac36092e79278"
     }
    },
    "a70b3e1e1a2c4d00a72ac242af8600af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4da1881f2a74b05ac8ec88563ca14dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d13fc0e4dee74587aafebc49bc194ad9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a70b3e1e1a2c4d00a72ac242af8600af",
      "placeholder": "​",
      "style": "IPY_MODEL_b4da1881f2a74b05ac8ec88563ca14dc",
      "value": "0.012 MB of 0.012 MB uploaded\r"
     }
    },
    "e1a342ab401a4616a02b0ebbdd5d0534": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
